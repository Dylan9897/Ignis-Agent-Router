# encoding : utf-8 -*-                            
# @author  : å†¬ç“œ                              
# @mail    : dylan_han@126.com    
# @Time    : 2026/1/20 17:00
# core/worker.py
import os
import threading
import traceback
import pyaudio
import dashscope
import requests
from http import HTTPStatus
from PyQt6.QtCore import pyqtSignal, QThread

from config import (
    DASHSCOPE_API_KEY, SAMPLE_RATE, CHANNELS, CHUNK_SIZE, 
    MODEL_ASR, MODEL_LLM, MODEL_TTS
)
from core.audio import AudioPlayer
from core.callbacks import ConversationCallback
from dashscope.audio.asr import Recognition
from dashscope.audio.tts import SpeechSynthesizer

# å¯¼å…¥æŠ€èƒ½æ¨¡å—
from skills.action_generate_llm_response import GenerateLLMResponse
from skills.server import get_intent

# è®¾ç½® API KEY
dashscope.api_key = DASHSCOPE_API_KEY

# Rasa NLU æœåŠ¡é…ç½®
RASA_SESSION_ID = "frog_ai_session"


class ConversationWorker(QThread):
    # å®šä¹‰ä¿¡å·ï¼šçŠ¶æ€å˜æ›´é€šçŸ¥ UI
    sig_state = pyqtSignal(str)  # IDLE, LISTENING, SPEAKING

    def __init__(self):
        super().__init__()
        self.active = False
        self.pa = pyaudio.PyAudio()
        self.vad_event = threading.Event()
        self.user_input_buffer = ""
        self.interrupt_event = threading.Event()  # æ‰“æ–­äº‹ä»¶
        self.current_state = "IDLE"  # è·Ÿè¸ªå½“å‰çŠ¶æ€
        # åˆå§‹åŒ– AudioPlayerï¼Œä¼ å…¥æ‰“æ–­äº‹ä»¶ä»¥ä¾¿å®æ—¶æ£€æŸ¥
        self.player = AudioPlayer(interrupt_event=self.interrupt_event)
        
        # åˆå§‹åŒ– LLM æœåŠ¡ï¼ˆç”¨äºæ§½ä½æå–å’Œé—²èŠï¼‰
        self.llm_service = GenerateLLMResponse()

    def stop(self):
        self.active = False
        self.quit()
        self.wait()

    def _listen_for_interrupt(self):
        """åœ¨SPEAKINGçŠ¶æ€æ—¶ç›‘å¬ç”¨æˆ·è¯­éŸ³ï¼Œæ£€æµ‹æ‰“æ–­"""
        try:
            from core.callbacks import InterruptCallback
            
            callback = InterruptCallback(self)
            mic_stream = None
            recognition = None

            # å¯åŠ¨ ASR ç”¨äºæ‰“æ–­æ£€æµ‹
            recognition = Recognition(
                model=MODEL_ASR,
                format='pcm',
                sample_rate=SAMPLE_RATE,
                callback=callback
            )
            recognition.start()

            # æ‰“å¼€éº¦å…‹é£
            mic_stream = self.pa.open(
                format=pyaudio.paInt16,
                channels=CHANNELS,
                rate=SAMPLE_RATE,
                input=True,
                frames_per_buffer=CHUNK_SIZE
            )

            # å½•éŸ³å¾ªç¯ - æŒç»­ç›‘å¬ç›´åˆ°SPEAKINGçŠ¶æ€ç»“æŸæˆ–æ£€æµ‹åˆ°æ‰“æ–­
            while self.current_state == "SPEAKING" and not self.interrupt_event.is_set():
                data = mic_stream.read(CHUNK_SIZE, exception_on_overflow=False)
                recognition.send_audio_frame(data)

        except Exception as e:
            print(f"âš ï¸ [Interrupt] æ‰“æ–­ç›‘å¬å‡ºé”™: {e}")
        finally:
            if mic_stream:
                mic_stream.stop_stream()
                mic_stream.close()
            if recognition:
                try:
                    recognition.stop()
                except:
                    pass

    def run(self):
        if "sk-" not in DASHSCOPE_API_KEY:
            print("âŒ é”™è¯¯: æœªè®¾ç½® API KEY")
            return

        self.active = True
        print("[System] æ ¸å¿ƒçº¿ç¨‹å¯åŠ¨")

        while self.active:
            # === 1. è†å¬é˜¶æ®µ ===
            is_interrupted = self.interrupt_event.is_set()
            
            if not is_interrupted:
                self.current_state = "LISTENING"
                self.sig_state.emit("LISTENING")
                self.vad_event.clear()
                self.user_input_buffer = ""

                callback = ConversationCallback(self)
                mic_stream = None
                recognition = None

                try:
                    # å¯åŠ¨ ASR
                    recognition = Recognition(
                        model=MODEL_ASR,
                        format='pcm',
                        sample_rate=SAMPLE_RATE,
                        callback=callback
                    )
                    recognition.start()

                    # æ‰“å¼€éº¦å…‹é£
                    mic_stream = self.pa.open(
                        format=pyaudio.paInt16,
                        channels=CHANNELS,
                        rate=SAMPLE_RATE,
                        input=True,
                        frames_per_buffer=CHUNK_SIZE
                    )

                    # å½•éŸ³å¾ªç¯
                    while self.active:
                        data = mic_stream.read(CHUNK_SIZE, exception_on_overflow=False)
                        recognition.send_audio_frame(data)
                        # å¦‚æœ VAD æ£€æµ‹åˆ°è¯´è¯ç»“æŸï¼Œè·³å‡ºå¾ªç¯
                        if self.vad_event.is_set():
                            break
                except Exception as e:
                    print(f"âŒ [Error] å½•éŸ³é˜¶æ®µå‡ºé”™: {e}")
                    traceback.print_exc()
                finally:
                    if mic_stream:
                        mic_stream.stop_stream()
                        mic_stream.close()
                    if recognition:
                        try:
                            recognition.stop()
                        except:
                            pass

                # å¦‚æœåœæ­¢äº†æˆ–æ²¡å¬åˆ°å£°éŸ³ï¼Œé‡æ–°å¾ªç¯
                if not self.active: break
                if not self.user_input_buffer: continue
            else:
                print("\nâš¡ [System] æ£€æµ‹åˆ°æ‰“æ–­ï¼Œè·³è¿‡ç›‘å¬ï¼Œç›´æ¥å¤„ç†æ–°è¾“å…¥")

            # === 2. æ€è€ƒä¸å›ç­”é˜¶æ®µ ===
            self.current_state = "SPEAKING"
            self.sig_state.emit("SPEAKING")
            print(f"\n[User] {self.user_input_buffer}")

            # æ¸…é™¤æ‰“æ–­æ ‡å¿—ï¼Œå‡†å¤‡æ–°ä¸€è½®æ’­æŠ¥
            self.interrupt_event.clear()
            
            # å¯åŠ¨æ‰“æ–­ç›‘å¬çº¿ç¨‹
            interrupt_thread = threading.Thread(target=self._listen_for_interrupt, daemon=True)
            interrupt_thread.start()

            user_query = self.user_input_buffer
            self.process_with_intent_routing(user_query)
            
            # ç­‰å¾…æ’­æ”¾å®Œæˆæˆ–è¢«æ‰“æ–­
            self.player.wait_until_done()
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­
            if self.interrupt_event.is_set():
                print("\nâš¡ [System] æ£€æµ‹åˆ°æ‰“æ–­ï¼Œç«‹å³åœæ­¢æ’­æŠ¥")
                self.player.stop()
            
            # çŠ¶æ€å›åˆ°ç©ºé—²
            self.current_state = "IDLE"

    def process_with_intent_routing(self, text):
        """
        æ ¸å¿ƒæµç¨‹ï¼šæ„å›¾è·¯ç”± + åˆ†æ”¯å¤„ç†
        1. è°ƒç”¨ Rasa-Pro è¯†åˆ«æ„å›¾
        2. æ ¹æ®æ„å›¾ç±»å‹åˆ†æ”¯å¤„ç†
        """
        try:
            # === æ­¥éª¤1: è°ƒç”¨ Rasa-Pro è·å–æ„å›¾ ===
            print(f"\nğŸ” [Rasa] æ­£åœ¨è¯†åˆ«æ„å›¾: {text}")
            
            try:
                rasa_result = get_intent(RASA_SESSION_ID, text)
                print(f"[Rasa] è¿”å›ç»“æœ: {rasa_result}")
                
                intent_name = rasa_result.get("intent", {}).get("name", "")
                confidence = rasa_result.get("intent", {}).get("confidence", 0)
                print(f"[Rasa] æ„å›¾: {intent_name}, ç½®ä¿¡åº¦: {confidence:.2f}")
                
            except Exception as e:
                print(f"âš ï¸ [Rasa] è¿æ¥å¤±è´¥ï¼Œé™çº§åˆ°é—²èŠæ¨¡å¼: {e}")
                intent_name = "chitchat"
                confidence = 0
            
            # === æ­¥éª¤2: æ ¹æ®æ„å›¾åˆ†æ”¯å¤„ç† ===
            
            # é—²èŠæ„å›¾ - ç›´æ¥è°ƒç”¨å¤§æ¨¡å‹
            if intent_name == "chitchat" or confidence < 0.5:
                print(f"ğŸ’¬ [é—²èŠæ¨¡å¼] è°ƒç”¨å¤§æ¨¡å‹å¯¹è¯")
                self._handle_chitchat(text)
                return
            
            # å‘é€å¾®ä¿¡æ¶ˆæ¯æ„å›¾
            if intent_name == "send_wechat_message":
                print(f"ğŸ“± [å¾®ä¿¡æ¨¡å¼] å¤„ç†å‘é€å¾®ä¿¡è¯·æ±‚")
                self._handle_send_wechat(text)
                return
            
            # æ§åˆ¶PPTæ„å›¾
            if intent_name == "control_ppt":
                print(f"ğŸ“Š [PPTæ¨¡å¼] å¤„ç†PPTæ§åˆ¶è¯·æ±‚")
                self._handle_control_ppt(text)
                return
            
            # æœç´¢æ–‡ä»¶æ„å›¾
            if intent_name == "search_file":
                print(f"ğŸ“ [æ–‡ä»¶æ¨¡å¼] å¤„ç†æ–‡ä»¶æœç´¢è¯·æ±‚")
                self._handle_search_file(text)
                return
            
            # å…¶ä»–æœªè¯†åˆ«æ„å›¾ï¼Œé™çº§åˆ°é—²èŠ
            print(f"â“ [æœªçŸ¥æ„å›¾] {intent_name}ï¼Œé™çº§åˆ°é—²èŠæ¨¡å¼")
            self._handle_chitchat(text)
            
        except Exception as e:
            print(f"\nâŒ [Error] æ„å›¾è·¯ç”±å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            self._speak_text("æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

    def _handle_chitchat(self, text):
        """å¤„ç†é—²èŠæ„å›¾ - è°ƒç”¨å¤§æ¨¡å‹æµå¼å›å¤"""
        self._call_llm_streaming(text)

    def _handle_send_wechat(self, text):
        """
        å¤„ç†å‘é€å¾®ä¿¡æ„å›¾
        1. è°ƒç”¨å¤§æ¨¡å‹æå–æ§½ä½ï¼ˆè”ç³»äººã€æ¶ˆæ¯å†…å®¹ï¼‰
        2. æ˜ å°„è”ç³»äººå§“åï¼ˆå¤„ç†ASRé”™åˆ«å­—ï¼‰
        3. æ‰§è¡Œå¾®ä¿¡å‘é€
        4. æ’­æŠ¥é»˜è®¤è¯æœ¯
        """
        # 1. ä½¿ç”¨å¤§æ¨¡å‹æå–æ§½ä½
        contact_name, message_content = self.llm_service.extract_wechat_slots(text)
        
        if not contact_name or contact_name == "None":
            self._speak_text("æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…è¦å‘ç»™è°ï¼Œè¯·å†è¯´ä¸€éã€‚")
            return
        
        if not message_content:
            self._speak_text("æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…è¦å‘é€ä»€ä¹ˆå†…å®¹ï¼Œè¯·å†è¯´ä¸€éã€‚")
            return
        
        # 2. æ‰§è¡Œå¾®ä¿¡å‘é€
        print(f"ğŸ“± [å¾®ä¿¡] å‡†å¤‡å‘é€: è”ç³»äºº={contact_name}, æ¶ˆæ¯={message_content}")
        
        try:
            from skills.action_send_wechat import ActionSendWechat
            
            # åˆ›å»º Mock å¯¹è±¡æ¥æ‰§è¡Œ Action
            class MockTracker:
                def __init__(self, slots):
                    self.slots = slots
                def get_slot(self, key):
                    return self.slots.get(key)
            
            class MockDispatcher:
                def utter_message(self, text=None, **kwargs):
                    print(f"[å¾®ä¿¡Action] {text}")
            
            action = ActionSendWechat()
            tracker = MockTracker({
                "contact_name": contact_name,
                "message_content": message_content
            })
            dispatcher = MockDispatcher()
            
            events = action.run(dispatcher, tracker, {})
            
            # æ£€æŸ¥æ‰§è¡Œç»“æœ
            action_status = "success"
            for event in events:
                if hasattr(event, 'key') and event.key == "action_status":
                    action_status = event.value
            
            # 3. æ’­æŠ¥é»˜è®¤è¯æœ¯
            if action_status == "success":
                reply_text = f"å¥½çš„ï¼Œå·²ç»é€šçŸ¥{contact_name}äº†ã€‚"
            else:
                reply_text = f"æŠ±æ­‰ï¼Œå‘é€ç»™{contact_name}å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥å¾®ä¿¡æ˜¯å¦å·²ç™»å½•ã€‚"
            
        except Exception as e:
            print(f"âŒ [å¾®ä¿¡] æ‰§è¡Œå¤±è´¥: {e}")
            reply_text = "æŠ±æ­‰ï¼Œå‘é€å¾®ä¿¡æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
        
        self._speak_text(reply_text)

    def _handle_control_ppt(self, text):
        """
        å¤„ç†æ§åˆ¶PPTæ„å›¾
        1. ä½¿ç”¨å¤§æ¨¡å‹æå–å…³é”®è¯
        2. æŒ‰ç›¸å…³æ€§åŒ¹é…PPTæ–‡ä»¶
        3. æ‰§è¡ŒPPTæ“ä½œ
        4. æ’­æŠ¥é»˜è®¤è¯æœ¯
        """
        # 1. ä½¿ç”¨å¤§æ¨¡å‹æå–å…³é”®è¯
        keyword = self.llm_service.extract_file_keyword(text, "PPT")
        
        if not keyword:
            self._speak_text("æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…è¦æ‰“å¼€å“ªä¸ªPPTï¼Œè¯·å†è¯´ä¸€éã€‚")
            return
        
        # 2. æŒ‰ç›¸å…³æ€§æœç´¢PPTæ–‡ä»¶
        file_path, file_name = self.llm_service.search_ppt_by_relevance(keyword)
        
        if not file_path:
            self._speak_text(f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ…å«\"{keyword}\"çš„PPTæ–‡ä»¶ã€‚")
            return
        
        # 3. åˆ¤æ–­æ˜¯æ‰“å¼€è¿˜æ˜¯æ§åˆ¶æ“ä½œ
        play_keywords = ["æ’­æ”¾", "å…¨å±", "æ”¾æ˜ ", "å¼€å§‹", "å¯åŠ¨"]
        nav_keywords = ["ä¸‹ä¸€é¡µ", "ä¸Šä¸€é¡µ", "å", "å‰", "é€€å‡º", "ç»“æŸ"]
        
        is_play = any(word in text for word in play_keywords)
        is_nav = any(word in text for word in nav_keywords)
        
        try:
            import pyautogui
            import pygetwindow as gw
            import time
            
            if not is_nav:
                # æ‰“å¼€æˆ–æ’­æ”¾PPT
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ‰“å¼€
                existing_wins = [w for w in gw.getAllWindows() 
                               if keyword.lower() in w.title.lower() or "WPS æ¼”ç¤º" in w.title]
                
                if not existing_wins:
                    os.startfile(file_path)
                    time.sleep(3.0)
                
                # æ¿€æ´»çª—å£
                active_wins = [w for w in gw.getAllWindows() 
                              if keyword.lower() in w.title.lower() or "WPS æ¼”ç¤º" in w.title]
                if active_wins:
                    win = active_wins[0]
                    if win.isMinimized:
                        win.restore()
                    win.activate()
                    time.sleep(0.5)
                
                if is_play:
                    pyautogui.press('f5')
                    reply_text = f"å·²ä¸ºæ‚¨æ‰“å¼€å¹¶æ’­æ”¾{file_name}ã€‚"
                else:
                    reply_text = f"å·²ä¸ºæ‚¨æ‰“å¼€{file_name}ã€‚"
            else:
                # ç¿»é¡µæˆ–é€€å‡ºæ“ä½œ
                if "ä¸‹" in text or "å" in text:
                    pyautogui.press('right')
                    reply_text = "å¥½çš„ï¼Œä¸‹ä¸€é¡µã€‚"
                elif "ä¸Š" in text or "å‰" in text:
                    pyautogui.press('left')
                    reply_text = "å¥½çš„ï¼Œä¸Šä¸€é¡µã€‚"
                elif "é€€å‡º" in text or "ç»“æŸ" in text:
                    pyautogui.press('esc')
                    reply_text = "å¥½çš„ï¼Œå·²é€€å‡ºæ’­æ”¾ã€‚"
                else:
                    reply_text = "å¥½çš„ã€‚"
            
        except Exception as e:
            print(f"âŒ [PPT] æ‰§è¡Œå¤±è´¥: {e}")
            reply_text = "æŠ±æ­‰ï¼Œæ§åˆ¶PPTæ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
        
        self._speak_text(reply_text)

    def _handle_search_file(self, text):
        """
        å¤„ç†æœç´¢æ–‡ä»¶æ„å›¾
        1. ä½¿ç”¨å¤§æ¨¡å‹æå–å…³é”®è¯
        2. æŒ‰ç›¸å…³æ€§åŒ¹é…æ–‡ä»¶
        3. æ‰“å¼€æ–‡ä»¶
        4. æ’­æŠ¥é»˜è®¤è¯æœ¯
        """
        # 1. ä½¿ç”¨å¤§æ¨¡å‹æå–å…³é”®è¯
        keyword = self.llm_service.extract_file_keyword(text, "æ–‡ä»¶")
        
        if not keyword:
            self._speak_text("æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…è¦æŸ¥æ‰¾ä»€ä¹ˆæ–‡ä»¶ï¼Œè¯·å†è¯´ä¸€éã€‚")
            return
        
        # 2. æŒ‰ç›¸å…³æ€§æœç´¢æ–‡ä»¶
        file_path, file_name = self.llm_service.search_file_by_relevance(keyword)
        
        if not file_path:
            self._speak_text(f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°åŒ…å«\"{keyword}\"çš„æ–‡ä»¶ã€‚")
            return
        
        # 3. æ‰“å¼€æ–‡ä»¶
        try:
            os.startfile(file_path)
            reply_text = f"å·²ä¸ºæ‚¨æ‰“å¼€{file_name}ã€‚"
        except Exception as e:
            print(f"âŒ [æ–‡ä»¶] æ‰“å¼€å¤±è´¥: {e}")
            reply_text = f"æŠ±æ­‰ï¼Œæ‰“å¼€{file_name}æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
        
        self._speak_text(reply_text)

    def _call_llm_streaming(self, text):
        """æµå¼è°ƒç”¨é€šä¹‰åƒé—®å¤§æ¨¡å‹ï¼ˆé—²èŠæ¨¡å¼ï¼‰"""
        responses = dashscope.Generation.call(
            model=MODEL_LLM,
            prompt=text,
            stream=True,
            result_format='message'
        )

        buffer_text = ""
        full_text = ""
        punctuations = {',', 'ï¼Œ', '.', 'ã€‚', '?', 'ï¼Ÿ', '!', 'ï¼', ';', 'ï¼›'}

        for response in responses:
            # æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­
            if self.interrupt_event.is_set():
                print("\nâš¡ [LLM] æ£€æµ‹åˆ°æ‰“æ–­ï¼Œåœæ­¢ç”Ÿæˆ")
                break
            if not self.active: break
            
            if response.status_code == HTTPStatus.OK:
                content = response.output.choices[0]['message']['content']
                delta = content[len(full_text):]
                full_text = content
                if not delta: continue

                print(delta, end="", flush=True)

                buffer_text += delta
                for char in delta:
                    if char in punctuations:
                        if not self.interrupt_event.is_set():
                            self.synthesize_and_play(buffer_text)
                        buffer_text = ""
                        break

        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if buffer_text and self.active and not self.interrupt_event.is_set():
            self.synthesize_and_play(buffer_text)
        print()
    
    def _speak_text(self, text):
        """å°†æ–‡æœ¬æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†æ®µï¼Œé€å» TTS æ’­æ”¾"""
        print(f"[AI] {text}")
        punctuations = {',', 'ï¼Œ', '.', 'ã€‚', '?', 'ï¼Ÿ', '!', 'ï¼', ';', 'ï¼›'}
        buffer_text = ""
        
        for char in text:
            buffer_text += char
            if char in punctuations:
                if buffer_text.strip() and not self.interrupt_event.is_set():
                    self.synthesize_and_play(buffer_text)
                buffer_text = ""
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if buffer_text.strip() and self.active and not self.interrupt_event.is_set():
            self.synthesize_and_play(buffer_text)

    def synthesize_and_play(self, text):
        if not text.strip(): 
            return
        try:
            result = SpeechSynthesizer.call(
                model=MODEL_TTS,
                text=text,
                sample_rate=SAMPLE_RATE
            )
            if result.get_audio_data():
                audio_data = result.get_audio_data()
                self.player.play(audio_data)
        except Exception as e:
            print(f"âŒ [TTS Error] {e}")
            traceback.print_exc()
