# encoding : utf-8 -*-                            
# @author  : å†¬ç“œ                              
# @mail    : dylan_han@126.com    
# @Time    : 2026/1/4 11:08
import os
import json
import logging
from datetime import datetime
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage


logger = logging.getLogger("BotLogger")

class LLMService:
    def __init__(self, config):
        # åˆå§‹åŒ– LangChain çš„ Chat æ¨¡å‹
        # ç»§ç»­ä½¿ç”¨ settings.yaml ä¸­çš„é…ç½®
        self.llm = ChatOpenAI(
            model=config['llm'].get('generation_model', 'qwen-plus'),
            api_key=os.getenv("ALI_API_KEY"),
            base_url=os.getenv("ALI_BASE_URL"),
            temperature=config['app'].get('temperature', 0.1),
            streaming=True,  # å¼€å¯æµå¼æ”¯æŒ
            max_tokens=50    # æ„å›¾è¯†åˆ«åªéœ€è¦è¾“å‡ºå‡ ä¸ªå­—
        )
        
        # ä¹Ÿå¯ä»¥å•ç‹¬åˆå§‹åŒ–ä¸€ä¸ª intent_llmï¼Œå¦‚æœéœ€è¦ä¸åŒé…ç½®
        self.intent_llm = ChatOpenAI(
            model=config['llm'].get('intent_model', 'qwen-turbo'),
            api_key=os.getenv("ALI_API_KEY"),
            base_url=os.getenv("ALI_BASE_URL"),
            temperature=0.0  # æ„å›¾è¯†åˆ«æ¸©åº¦è®¾ä¸º0
        )

    def get_llm(self):
        """
        è¿”å›ç”¨äºç”Ÿæˆçš„é€šç”¨ LLM å¯¹è±¡
        ç»™ AgentEngine ç”Ÿæˆè¯æœ¯ç”¨
        """
        return self.llm

    def get_intent_llm(self):
        """
        è¿”å›ç”¨äºæ„å›¾è¯†åˆ«çš„å¿«é€Ÿ LLM å¯¹è±¡
        ç»™ IntentRouter ç”¨
        """
        return self.intent_llm

    def generate_response(self, system_instruction,**kwargs):
        """
        æµå¼ç”Ÿæˆå‚¬æ”¶è¯æœ¯ï¼šé€ token è¿”å›ï¼Œä¾›å®æ—¶æ¶ˆè´¹ã€‚
        è°ƒç”¨æ–¹åº”è¿­ä»£æ­¤å‡½æ•°è¿”å›å€¼ä»¥è·å–æµå¼è¾“å‡ºã€‚

        Args:
            system_instruction: ç”¨æˆ·æŒ‡ä»¤ï¼ˆå«æ§½ä½å¡«å……åçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰

        Yields:
            str: æ¯æ¬¡ç”Ÿæˆçš„ä¸€ä¸ª tokenï¼ˆæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œéœ€è¿‡æ»¤ï¼‰
        """
        try:
            model_choice = kwargs.get("model_choice",None)
            if not model_choice:
                model_choice = self.default_model
            logger.info(f"Using Generation Model: {model_choice}")
            stream = self.client.chat.completions.create(
                model=model_choice,
                messages=[
                    {
                        'role': 'system',
                        'content': (
                            "ä½ æ˜¯ä¸€åä¸“ä¸šã€åˆè§„çš„é‡‘èå‚¬æ”¶ä¸“å‘˜ã€‚"
                            "è¯·æ ¹æ®å®¢æˆ·æƒ…å†µç”Ÿæˆæ¸©å’Œä½†æ˜ç¡®çš„å‚¬æ”¶è¯æœ¯ï¼Œ"
                            "è¯­æ°”å°Šé‡ï¼Œä¸å¨èƒã€ä¸å¤¸å¤§ï¼Œæ¯å¥è¯ä¸è¶…è¿‡60å­—ã€‚"
                            "ä¸è¦åŠ å¼•å·ã€ä¸è¦è§£é‡Šï¼Œç›´æ¥è¾“å‡ºè¯æœ¯ã€‚"
                        )
                    },
                    {'role': 'user', 'content': system_instruction}
                ],
                stream=True,
                temperature=kwargs.get("temperature",0.5)  # å‚¬æ”¶å»ºè®®æ›´ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®š
            )

            for chunk in stream:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content is not None:
                    token = delta.content
                    yield token  # ğŸ‘ˆ å…³é”®ï¼šé€ token æµå¼äº§å‡º

        except Exception as e:
            logger.error(f"Streaming Generation Error: {e}")
            yield "[ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•]"