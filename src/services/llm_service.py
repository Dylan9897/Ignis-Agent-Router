# encoding : utf-8 -*-                            
# @author  : å†¬ç“œ                              
# @mail    : dylan_han@126.com    
# @Time    : 2026/1/4 11:08
import os
import json
import logging
from datetime import datetime
from openai import OpenAI

logger = logging.getLogger("BotLogger")

class LLMService:
    def __init__(self, config):
        self.client = OpenAI(
            api_key=os.getenv("ALI_API_KEY"),
            base_url=os.getenv("ALI_BASE_URL")
        )
        # self.intent_model = config['llm'].get('intent_model', 'qwen-turbo')
        self.default_model = config['llm'].get('generation_model', 'qwen-plus')

    def generate_response(self, system_instruction,**kwargs):
        """
        æµå¼ç”Ÿæˆå‚¬æ”¶è¯æœ¯ï¼šé€ token è¿”å›ï¼Œä¾›å®æ—¶æ¶ˆè´¹ã€‚
        è°ƒç”¨æ–¹åº”è¿­ä»£æ­¤å‡½æ•°è¿”å›å€¼ä»¥è·å–æµå¼è¾“å‡ºã€‚

        Args:
            system_instruction: ç”¨æˆ·æŒ‡ä»¤ï¼ˆå«æ§½ä½å¡«å……åçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰

        Yields:
            str: æ¯æ¬¡ç”Ÿæˆçš„ä¸€ä¸ª tokenï¼ˆæˆ–ç©ºå­—ç¬¦ä¸²ï¼Œéœ€è¿‡æ»¤ï¼‰
        """
        try:
            model_choice = kwargs.get("model_choice",None)
            if not model_choice:
                model_choice = self.default_model
            logger.info(f"Using Generation Model: {model_choice}")
            stream = self.client.chat.completions.create(
                model=model_choice,
                messages=[
                    {
                        'role': 'system',
                        'content': (
                            "ä½ æ˜¯ä¸€åä¸“ä¸šã€åˆè§„çš„é‡‘èå‚¬æ”¶ä¸“å‘˜ã€‚"
                            "è¯·æ ¹æ®å®¢æˆ·æƒ…å†µç”Ÿæˆæ¸©å’Œä½†æ˜ç¡®çš„å‚¬æ”¶è¯æœ¯ï¼Œ"
                            "è¯­æ°”å°Šé‡ï¼Œä¸å¨èƒã€ä¸å¤¸å¤§ï¼Œæ¯å¥è¯ä¸è¶…è¿‡60å­—ã€‚"
                            "ä¸è¦åŠ å¼•å·ã€ä¸è¦è§£é‡Šï¼Œç›´æ¥è¾“å‡ºè¯æœ¯ã€‚"
                        )
                    },
                    {'role': 'user', 'content': system_instruction}
                ],
                stream=True,
                temperature=kwargs.get("temperature",0.5)  # å‚¬æ”¶å»ºè®®æ›´ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®š
            )

            for chunk in stream:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content is not None:
                    token = delta.content
                    yield token  # ğŸ‘ˆ å…³é”®ï¼šé€ token æµå¼äº§å‡º

        except Exception as e:
            logger.error(f"Streaming Generation Error: {e}")
            yield "[ç³»ç»Ÿé”™è¯¯ï¼Œè¯·ç¨åé‡è¯•]"